{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c44bc7-58dc-49ca-a0a3-05fcbfffeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import requests\n",
    "from config import SETTINGS\n",
    "from geo.Geoserver import Geoserver\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ca7df-b1ae-4645-80c0-3844230b8e54",
   "metadata": {},
   "source": [
    "# Upload NIVA datasets\n",
    "\n",
    "Files within the `niva` bucket are arranged in a fairly complex hierarchy. This can probably be simplified, but it's going to take a while.\n",
    "\n",
    "This notebook searches a \"mission\" folder for orthomosaics with user-specified names. For example, Hege sent me a list of mosaic names for the Kelpmap project (which I hadn't previously found because they're quite deeply buried). The orginal file names are generally not very helpful, so in this notebook the user must specify more suitable names for each file. This is done manually. The code then performs the following operations:\n",
    "\n",
    " 1. Finds the files on MinIO\n",
    " 2. Builds 3-band COGs for all datasets\n",
    " 3. Uploads the files to GeoServer\n",
    " 4. Publishes them to GeoNode\n",
    " 5. Updates the basic metadata by extracting information from the user-specified file names\n",
    " \n",
    "## 1. Find files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63475b6-4692-461a-addc-2d9967edae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_by_name(fname, parent_folder):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        if fname in files:\n",
    "            result.append(os.path.join(root, fname))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e4f9f-832f-4f2d-8342-0952bdebc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission folder to search\n",
    "base_dir = r\"/home/notebook/shared-seabee-ns9879k/niva/2022/2022-08-18_KELPMAP_Vega/\"\n",
    "\n",
    "# Input files to search for, mapped to more helpful output file names:\n",
    "#     project_region_area_org_spec_date-time.tif\n",
    "fnames_dict = {\n",
    "    \"KelpMap_N_20220819_MS_comp.tif\": \"KELPMAP_Vega_North_Spectrofly_MS_20220819-0000.tif\",\n",
    "    \"20220819_1230_RGB_120m_transparent_mosaic_group1.tif\": \"KELPMAP_Vega_North_Spectrofly_RGB_20220819-1230.tif\",\n",
    "    \"KelpMap_S_20220818_MS_v2.tif\": \"KELPMAP_Vega_South_Spectrofly_MS_20220818-0000.tif\",\n",
    "    \"1055rgb120_transparent_mosaic_group1.tif\": \"KELPMAP_Vega_North_NIVA_RGB_20220818-1055.tif\",\n",
    "    \"0944_ms_120_composite.tif\": \"KELPMAP_Vega_North_NIVA_MS_20220819-0944.tif\",\n",
    "    \"0814_rgb_115_transparent_mosaic_group1.tif\": \"KELPMAP_Vega_South_NIVA_RGB_20220819-0814.tif\",\n",
    "    \"0814_ms_115_composite.tif\": \"KELPMAP_Vega_South_NIVA_MS_20220819-0814.tif\",\n",
    "    \"1012_rgb_60_transparent_mosaic_group1.tif\": \"KELPMAP_Vega_South_NIVA_RGB_20220819-1012.tif\",\n",
    "}\n",
    "\n",
    "cog_fold = r\"/home/notebook/cogs/\"\n",
    "n_threads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f664aca-7d9c-4e00-80c4-ad2fd4b97851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output names are unique\n",
    "assert len(set(fnames_dict.values())) == len(fnames_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a4318-e93e-4a46-b522-b8ad1c65270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = []\n",
    "for fname in fnames_dict.keys():\n",
    "    fpaths = find_file_by_name(fname, base_dir)\n",
    "    if len(fpaths) > 1:\n",
    "        print(fpaths)\n",
    "    elif len(fpaths) == 0:\n",
    "        print(\"Could not find:\", fname)\n",
    "    else:\n",
    "        flist.append(fpaths[0])\n",
    "# flist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55450a1f-ed01-4be5-8a84-63186a6a3c6a",
   "metadata": {},
   "source": [
    "## 2. Convert to COGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fcd90-2b48-43f7-b2f6-7bfc7fedf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fpath in flist:\n",
    "    fname = os.path.basename(fpath)\n",
    "    cog_path = os.path.join(cog_fold, fnames_dict[fname])\n",
    "    cmd = [\n",
    "        \"gdal_translate\",\n",
    "        \"-b\",\n",
    "        \"1\",\n",
    "        \"-b\",\n",
    "        \"2\",\n",
    "        \"-b\",\n",
    "        \"3\",\n",
    "        \"-of\",\n",
    "        \"COG\",\n",
    "        \"-ot\",\n",
    "        \"Byte\",\n",
    "        \"-co\",\n",
    "        \"COMPRESS=LZW\",\n",
    "        \"-co\",\n",
    "        \"PREDICTOR=2\",\n",
    "        \"-co\",\n",
    "        f\"NUM_THREADS={n_threads}\",\n",
    "        \"-co\",\n",
    "        \"OVERVIEWS=IGNORE_EXISTING\",\n",
    "        \"-co\",\n",
    "        \"BIGTIFF=YES\",\n",
    "        \"-scale\",\n",
    "        \"-a_nodata\",\n",
    "        \"0\",\n",
    "        fpath,\n",
    "        cog_path,\n",
    "    ]\n",
    "    subprocess.check_call(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dba7c1-abe6-4866-bec8-5f6b8067304f",
   "metadata": {},
   "source": [
    "## 3. Upload to GeoServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173dc35-d746-449e-9cfc-cc16fa8fcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authernticate with GeoServer\n",
    "geo = Geoserver(\n",
    "    \"https://geonode.seabee.sigma2.no/geoserver\",\n",
    "    username=SETTINGS.GEOSERVER_USER,\n",
    "    password=SETTINGS.GEOSERVER_PASSWORD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1abc4-b853-4297-bc61-02770dcf2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload COGs to GeoServer\n",
    "workspace = \"geonode\"\n",
    "\n",
    "search_path = os.path.join(cog_fold, \"*.tif\")\n",
    "flist = glob(search_path)\n",
    "for fpath in tqdm(flist):\n",
    "    fname = os.path.basename(fpath)\n",
    "    layer_name = os.path.splitext(fname)[0]\n",
    "\n",
    "    # Add to GeoServer. Note: Will overwrite layer if it exists\n",
    "    status = geo.create_coveragestore(\n",
    "        layer_name=layer_name, path=fpath, workspace=workspace\n",
    "    )\n",
    "    # print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11704b3a-24e9-43b0-a930-65182cc553f6",
   "metadata": {},
   "source": [
    "## 4. Update GeoNode\n",
    "\n",
    "Trigger the `updatelayers` command for each new layer via the GeoNode API.\n",
    "\n",
    "Alternatively, this can be done manually by logging in to the GeoNode administration panel and navigatinge to\n",
    "\n",
    "    Home > Management Commands Over HTTP > Management command jobs\n",
    "    \n",
    "Choose `Add management command job` and set the **Command** to `updatelayers`. Check the **Autostart** box and click **Save**. If you have added a lot of data, the update process may take a while. When it is finished, the status should be updated and the new images datasets be visible in GeoNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115953b2-6f58-4990-88d6-1b8e97889491",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cog_fold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m auth \u001b[39m=\u001b[39m (SETTINGS\u001b[39m.\u001b[39mGEOSERVER_USER, SETTINGS\u001b[39m.\u001b[39mGEONODE_PASSWORD)\n\u001b[1;32m      8\u001b[0m wait \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m search_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cog_fold, \u001b[39m\"\u001b[39m\u001b[39m*.tif\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m flist \u001b[39m=\u001b[39m glob(search_path)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m fpath \u001b[39min\u001b[39;00m tqdm(flist):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cog_fold' is not defined"
     ]
    }
   ],
   "source": [
    "base_url = \"https://geonode.seabee.sigma2.no/api/v2/\"\n",
    "cmd_url = base_url + r\"management/commands/\"\n",
    "status_url = base_url + r\"management/jobs/\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "auth = (SETTINGS.GEOSERVER_USER, SETTINGS.GEONODE_PASSWORD)\n",
    "\n",
    "wait = 10  # seconds\n",
    "\n",
    "search_path = os.path.join(cog_fold, \"*.tif\")\n",
    "flist = glob(search_path)\n",
    "for fpath in tqdm(flist):\n",
    "    fname = os.path.basename(fpath)\n",
    "    layer_name = os.path.splitext(fname)[0]\n",
    "\n",
    "    # Update layer\n",
    "    command = \"updatelayers\"\n",
    "    kwargs = {\"filter\": layer_name, \"store\": layer_name, \"workspace\": \"geonode\"}\n",
    "    response = requests.post(\n",
    "        cmd_url,\n",
    "        headers=headers,\n",
    "        auth=auth,\n",
    "        data=json.dumps({\"command\": command, \"kwargs\": kwargs}),\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Optional: Wait for completion. Comment out this block to queue all\n",
    "    # update operations. You can then just wait for the batch to finish\n",
    "    # (e.g. check via the admin. panel)\n",
    "    job_id = response.json()[\"data\"][\"id\"]\n",
    "    job_url = status_url + f\"{job_id}/status/\"\n",
    "    job_status = \"NOT_FINISHED\"\n",
    "    while job_status != \"FINISHED\":\n",
    "        job_status = requests.get(job_url, auth=auth).json()[\"status\"]\n",
    "        time.sleep(wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6ea8f-d775-4ea1-b1e7-ada0c41c51ba",
   "metadata": {},
   "source": [
    "## 5. Update metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ad40b-0aa3-49cd-b391-8bd9badb92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://geonode.seabee.sigma2.no/api/v2/\"\n",
    "headers = {\"Authorization\": f\"Bearer {SETTINGS.GEONODE_TOKEN}\"}\n",
    "auth = (SETTINGS.GEOSERVER_USER, SETTINGS.GEONODE_PASSWORD)\n",
    "\n",
    "inv_fnames_dict = {y: x for x, y in fnames_dict.items()}\n",
    "search_path = os.path.join(cog_fold, \"*.tif\")\n",
    "flist = glob(search_path)\n",
    "for fpath in tqdm(flist):\n",
    "    fname = os.path.basename(fpath)\n",
    "    layer_name = os.path.splitext(fname)[0]\n",
    "\n",
    "    # Find resource ID\n",
    "    filter_url = base_url + f\"resources?search={layer_name}&search_fields=title\"\n",
    "    response = requests.request(\"GET\", filter_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    assert data[\"total\"] == 1, f\"More than one dataset found with title '{layer_name}'.\"\n",
    "    dataset_id = data[\"resources\"][0][\"pk\"]\n",
    "\n",
    "    # Extract metadata from names with format:\n",
    "    #    project_region_area_org_spec_date-time.tif\n",
    "    project, area, ns, org, bands, date = layer_name.split(\"_\")\n",
    "    date = dt.datetime.strptime(date, \"%Y%m%d-%H%M\")\n",
    "    orig_fname = inv_fnames_dict[fname]\n",
    "    abstract = (\n",
    "        f\"{bands} mosaic collected by {org} for the {project} project at {area} {ns} on {date}.\"\n",
    "        f\"<br><br><b>MinIO file name:</b> {orig_fname}.\"\n",
    "    )\n",
    "\n",
    "    # Update metadata\n",
    "    data = {\n",
    "        \"abstract\": abstract,\n",
    "        \"date\": date.isoformat(),\n",
    "        \"date_type\": \"creation\",\n",
    "        \"attribution\": \"SeaBee\",\n",
    "    }\n",
    "    update_url = base_url + f\"datasets/{dataset_id}\"\n",
    "    response = requests.patch(update_url, auth=auth, json=data)\n",
    "    response.raise_for_status()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "412ab0ef",
   "metadata": {},
   "source": [
    "## Update keywords and ISO fields\n",
    "\n",
    "Geonode have a rest API for datasets on `/api/v2/datasets`, but updating `keywords` and `tkeywords` does not seem to work. One (temporary) approach to achieve it is to:\n",
    "\n",
    "1. Get the dataset using the geonode api\n",
    "2. Get the full iso `MD_Metadata` using the csw endpoint `/catalogue/csw`\n",
    "    - note pycsw does not support transactions for geonode\n",
    "3. Modify the iso record with seabeepy's gmd package or lxml would also work\n",
    "4. Login with a csrf token and upload the iso file on `/datasets/upload`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4eb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from seabeepy.metadata import utils, templates, gmd\n",
    "from typing import List, Dict, Tuple\n",
    "# used to marshall the metadata xml\n",
    "from xsdata.formats.dataclass import serializers\n",
    "# ignore owslib future warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56820685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_iso_metadata(ds_metadata: List[Tuple[str,gmd.MdMetadata]], geonode_url) -> List[Dict[str,str]]:\n",
    "    \"\"\"Upload iso for a list of datasets\n",
    "\n",
    "    Login to geonode and get a csrf token so we are allowed to post to `dataset/upload`\n",
    "    \"\"\"\n",
    "    serializer = serializers.XmlSerializer()\n",
    "    response_list = []\n",
    "    login_url = f\"{geonode_url}/account/login/\"\n",
    "    def post_metadata(client, title:str, ds_meta: gmd.MdMetadata, csrftoken: str):\n",
    "        return client.post(\n",
    "            f\"{geonode_url}/datasets/upload\",\n",
    "            files={\n",
    "                \"base_file\": (\n",
    "                    \"sample.xml\",\n",
    "                    serializer.render(ds_meta),\n",
    "                    \"text/xml\",\n",
    "                ),\n",
    "            },\n",
    "            data={\n",
    "                \"permissions\": \"{}\",\n",
    "                \"charset\": \"undefined\",\n",
    "                \"metadata_upload_form\": \"true\",\n",
    "                \"dataset_title\": title,\n",
    "            },\n",
    "            headers={\n",
    "                \"X-CSRFToken\": csrftoken,\n",
    "                \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "            },\n",
    "            cookies={\"csrftoken\": csrftoken}\n",
    "            )\n",
    "    \n",
    "    client = requests.session()\n",
    "    client.get(login_url)\n",
    "    # Django would like the csrf token passed with the data, so we do need to save it off seperately.\n",
    "    csrftoken = client.cookies[\"csrftoken\"]\n",
    "    r = client.post(\n",
    "        login_url, data={\"login\": SETTINGS.GEONODE_USER, \"password\": SETTINGS.GEONODE_PASSWORD, \"csrfmiddlewaretoken\": csrftoken}\n",
    "    )\n",
    "    # For some reason, we are issued a new csrf token after logging in, so update your local copy.\n",
    "    csrftoken = client.cookies[\"csrftoken\"]\n",
    "    \n",
    "    for title, ds_meta in ds_metadata:\n",
    "        resp = post_metadata(client, title, ds_meta, csrftoken)\n",
    "        response_list.append(resp)\n",
    "    return response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919269a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using some older and unregular routes:)\n",
    "# So not part of v2 api\n",
    "geonode_url = \"http://localhost\"\n",
    "# setup xml parsers\n",
    "serializer = serializers.XmlSerializer(config=serializers.config.SerializerConfig(pretty_print=True))\n",
    "\n",
    "resp = requests.get(f\"{geonode_url}/api/v2/datasets\").json()\n",
    "datasets = [ds for ds in resp[\"datasets\"] if ds[\"subtype\"] == \"raster\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45839276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIMS Title\n",
      "NEW NEW ABSTRACT\n"
     ]
    }
   ],
   "source": [
    "# Keep the list of metadata instance\n",
    "# this also allow us to marshall all of them to disk\n",
    "# We add the title(`alternate`) from the ds along with iso record\n",
    "ds_metadata_list = []\n",
    "\n",
    "for ds in datasets:\n",
    "    ds_meta = utils.fetch_dataset_iso(datasets[0][\"uuid\"], geonode_url)\n",
    "    print(ds_meta.identification_info[0].md_data_identification.citation.ci_citation.title.character_string)\n",
    "    print(ds_meta.identification_info[0].md_data_identification.abstract.character_string)\n",
    "    ds_meta = utils.remove_all_keywords(ds_meta)\n",
    "    # Could also just remove norwegian keywords if keeping custom keywords\n",
    "    # ds_meta = utils.remove_norwegian_thesarus(ds_meta)\n",
    "    # Add norwegian keywords\n",
    "    # See https://register.geonorge.no/metadata-kodelister/inspiretema\n",
    "    ds_meta = utils.add_norwegian_thesarus_keywords(ds_meta, [\"Ortofoto\", \"Habitater og biotoper\"])\n",
    "    # Add custom seabee keywords\n",
    "    ds_meta = utils.add_seabee_keywords(ds_meta, [\"SeaBee\", \"NIVA\"])\n",
    "    ds_metadata_list.append((ds[\"alternate\"], ds_meta))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7695e58",
   "metadata": {},
   "source": [
    "### We can marshall the python object to get the xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8cbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sample.xml\", \"w\") as f:\n",
    "    f.write(serializer.render(ds_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaf49d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_list = upload_iso_metadata(ds_metadata_list, geonode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e96fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finished']\n",
      "http://localhost/catalogue/#/dataset/4\n"
     ]
    }
   ],
   "source": [
    "for r in resp_list:\n",
    "    res = r.json()\n",
    "    print(res[\"status\"])\n",
    "    print(f\"{geonode_url}{res['url']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "69d0b74f1e5111a26363d8ed5035edb8ad5befc32b6ba89d070f1858b7178fd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
